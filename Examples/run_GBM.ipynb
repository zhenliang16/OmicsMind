{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acb54a14",
   "metadata": {},
   "source": [
    "# GBM Example\n",
    "\n",
    "\n",
    "This notebook implements the OmicsMind pipeline for GBM dataset:\n",
    "- `rna.csv`\n",
    "- `rppa.csv`\n",
    "- `methylation.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72eb8901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.9.1+cpu\n"
     ]
    }
   ],
   "source": [
    "# !pip -q install pandas numpy scikit-learn tqdm torch\n",
    "\n",
    "import os, random, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from omicsmind.data import load_omics_table, MultiOmicsDataset, compute_nrmse_numpy\n",
    "from omicsmind.omicsmind import ModalityVAE, CrossOmicsTransformer, OmicsMind, omicsmind_loss\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96a3ca9",
   "metadata": {},
   "source": [
    "## 1. Load data tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "760670de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rna loaded (32, 19660)\n",
      "rppa loaded (32, 158)\n",
      "meth loaded (32, 20114)\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"./data/GBM_train\"   \n",
    "files = {\n",
    "    \"rna\": \"rna.csv\",\n",
    "    \"rppa\": \"rppa.csv\",\n",
    "    \"meth\": \"meth.csv\",\n",
    "}\n",
    "\n",
    "modalities = {}\n",
    "missing_files = []\n",
    "for k, fn in files.items():\n",
    "    p = os.path.join(data_dir, fn)\n",
    "    if os.path.exists(p):\n",
    "        modalities[k] = load_omics_table(p)\n",
    "        print(k, \"loaded\", modalities[k].shape)\n",
    "    else:\n",
    "        missing_files.append(p)\n",
    "\n",
    "if missing_files:\n",
    "    print(\"\\n[WARN] Missing files, will use synthetic data:\", missing_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932002a3",
   "metadata": {},
   "source": [
    "## 2. If files are missing, generate synthetic data(Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e03bc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "if missing_files:\n",
    "    np.random.seed(0)\n",
    "    n = 80\n",
    "    modalities = {\n",
    "        \"rna\":  pd.DataFrame(np.random.randn(n, 2000), index=[f\"S{i:03d}\" for i in range(n)]),\n",
    "        \"rppa\": pd.DataFrame(np.random.randn(n, 200),  index=[f\"S{i:03d}\" for i in range(n)]),\n",
    "        \"meth\": pd.DataFrame(np.random.randn(n, 1000), index=[f\"S{i:03d}\" for i in range(n)]),\n",
    "    }\n",
    "    # Simulate block-wise missingness\n",
    "    idx = modalities[\"rna\"].index\n",
    "    mask_rppa = np.random.choice(idx, size=int(0.3*n), replace=False)\n",
    "    mask_meth = np.random.choice(idx, size=int(0.2*n), replace=False)\n",
    "    modalities[\"rppa\"].loc[mask_rppa] = np.nan\n",
    "    modalities[\"meth\"].loc[mask_meth] = np.nan\n",
    "\n",
    "    print(\"Synthetic modalities:\", {k:v.shape for k,v in modalities.items()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4668d29f",
   "metadata": {},
   "source": [
    "## 3. Sample alignment + z-score normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eb226bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rna': (32, 19660), 'rppa': (32, 158), 'meth': (32, 20114)}\n"
     ]
    }
   ],
   "source": [
    "omics_keys = list(files.keys())\n",
    "all_samples = sorted(set().union(*[modalities[k].index for k in omics_keys]))\n",
    "\n",
    "def align(df): \n",
    "    return df.reindex(all_samples)\n",
    "\n",
    "modalities_aligned = {k: align(modalities[k]) for k in omics_keys}\n",
    "\n",
    "def zscore(df):\n",
    "    mu = df.mean(axis=0, skipna=True)\n",
    "    sd = df.std(axis=0, skipna=True).replace(0, 1.0)\n",
    "    return (df - mu) / sd\n",
    "\n",
    "modalities_aligned = {k: zscore(v) for k,v in modalities_aligned.items()}\n",
    "\n",
    "X = {k: v.to_numpy(dtype=np.float32) for k,v in modalities_aligned.items()}\n",
    "print({k: X[k].shape for k in X})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc64492",
   "metadata": {},
   "source": [
    "## 4. Dataset for block-wise masked-modality training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a49422ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MultiOmicsDataset(X, mask_prob=0.3)\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06da24cd",
   "metadata": {},
   "source": [
    "## 6. Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70e7c704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | loss=2.9447 rec=2.9311 kl=13.6112\n",
      "Epoch 005 | loss=2.9359 rec=2.8949 kl=41.0143\n",
      "Epoch 010 | loss=2.8495 rec=2.8280 kl=21.4989\n",
      "Epoch 015 | loss=2.8688 rec=2.8453 kl=23.5506\n",
      "Epoch 020 | loss=2.7160 rec=2.6866 kl=29.3792\n",
      "Epoch 025 | loss=2.5942 rec=2.5509 kl=43.2452\n",
      "Epoch 030 | loss=2.6269 rec=2.5500 kl=76.9124\n",
      "Epoch 035 | loss=2.4293 rec=2.3491 kl=80.1967\n",
      "Epoch 040 | loss=2.3849 rec=2.2679 kl=116.9851\n",
      "Epoch 045 | loss=2.3041 rec=2.2175 kl=86.5792\n",
      "Epoch 050 | loss=2.1833 rec=2.0689 kl=114.4531\n",
      "Epoch 055 | loss=2.1107 rec=1.9836 kl=127.1076\n",
      "Epoch 060 | loss=2.1927 rec=2.0817 kl=111.0127\n",
      "Epoch 065 | loss=2.0562 rec=1.9294 kl=126.8001\n",
      "Epoch 070 | loss=2.1994 rec=2.0129 kl=186.5778\n",
      "Epoch 075 | loss=2.0266 rec=1.8758 kl=150.8167\n",
      "Epoch 080 | loss=1.8170 rec=1.6806 kl=136.4053\n",
      "Epoch 085 | loss=1.8349 rec=1.7157 kl=119.1924\n",
      "Epoch 090 | loss=1.7214 rec=1.5931 kl=128.2557\n",
      "Epoch 095 | loss=1.7098 rec=1.5807 kl=129.0930\n",
      "Epoch 100 | loss=1.4788 rec=1.3193 kl=159.5007\n",
      "Epoch 105 | loss=1.6571 rec=1.4690 kl=188.0061\n",
      "Epoch 110 | loss=1.4245 rec=1.2421 kl=182.4398\n",
      "Epoch 115 | loss=1.5836 rec=1.4221 kl=161.4419\n",
      "Epoch 120 | loss=1.4687 rec=1.3153 kl=153.4257\n",
      "Epoch 125 | loss=1.5392 rec=1.3581 kl=181.0501\n",
      "Epoch 130 | loss=1.2303 rec=1.0678 kl=162.5139\n",
      "Epoch 135 | loss=1.3250 rec=1.1886 kl=136.3633\n",
      "Epoch 140 | loss=1.2562 rec=1.1217 kl=134.5095\n",
      "Epoch 145 | loss=1.1127 rec=0.9589 kl=153.8458\n",
      "Epoch 150 | loss=1.0521 rec=0.9076 kl=144.4696\n",
      "Epoch 155 | loss=0.9946 rec=0.8613 kl=133.3915\n",
      "Epoch 160 | loss=0.9774 rec=0.8349 kl=142.4764\n",
      "Epoch 165 | loss=1.1697 rec=1.0167 kl=152.9332\n",
      "Epoch 170 | loss=1.1083 rec=0.9468 kl=161.4932\n",
      "Epoch 175 | loss=0.9670 rec=0.8089 kl=158.0759\n",
      "Epoch 180 | loss=0.9408 rec=0.8215 kl=119.3237\n",
      "Epoch 185 | loss=1.2727 rec=1.1003 kl=172.3358\n",
      "Epoch 190 | loss=0.8443 rec=0.6825 kl=161.7652\n",
      "Epoch 195 | loss=0.9854 rec=0.8350 kl=150.3622\n",
      "Epoch 200 | loss=1.1584 rec=1.0088 kl=149.5719\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "in_dims = {m: X[m].shape[1] for m in X}\n",
    "\n",
    "model = OmicsMind(in_dims, z_dim=64).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def to_tensor_batch(batch):\n",
    "    sample, observed, train_mask = batch\n",
    "    batch_x = {m: torch.tensor(sample[m], device=device) for m in sample}\n",
    "    observed_t = {m: torch.tensor(observed[m], device=device) for m in observed}\n",
    "    train_mask_t = {m: torch.tensor(train_mask[m], device=device) for m in train_mask}\n",
    "    return batch_x, observed_t, train_mask_t\n",
    "\n",
    "epochs = 200\n",
    "for ep in range(1, epochs+1):\n",
    "    model.train()\n",
    "    total = total_rec = total_kl = 0.0\n",
    "    for batch in loader:\n",
    "        bx, obs_t, mask_t = to_tensor_batch(batch)\n",
    "        refined_xhat, mus, logvars = model(bx, mask_t)\n",
    "        loss, rec, kl = omicsmind_loss(bx, refined_xhat, mus, logvars, obs_t, mask_t, only_masked=False)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total += loss.item(); total_rec += rec.item(); total_kl += kl.item()\n",
    "\n",
    "    if ep % 5 == 0 or ep == 1:\n",
    "        print(f\"Epoch {ep:03d} | loss={total/len(loader):.4f} rec={total_rec/len(loader):.4f} kl={total_kl/len(loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dea188",
   "metadata": {},
   "source": [
    "## 9. Full imputation and CSV export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b5e766c-8c7f-442b-9759-69c0ebb12b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOADING DATA TO IMPUTE (ORDER-PRESERVING)\n",
      "============================================================\n",
      "Data directory: ./data/GBM_raw\n",
      "\n",
      "✓ rna loaded from rna.csv, shape: (595, 19660)\n",
      "  → Block-missing rows:  443\n",
      "  → Total missing values: 8709380\n",
      "✓ rppa loaded from rppa.csv, shape: (595, 158)\n",
      "  → Block-missing rows:  363\n",
      "  → Total missing values: 57442\n",
      "✓ meth loaded from meth.csv, shape: (595, 20114)\n",
      "  → Block-missing rows:  457\n",
      "  → Total missing values: 9193530\n",
      "\n",
      "Total samples to impute: 595\n",
      "Sample order preserved.\n",
      "\n",
      "============================================================\n",
      "EXTRACTING TRAINING NORMALIZATION PARAMS\n",
      "============================================================\n",
      "rna: mean/std loaded (shape (19660,))\n",
      "rppa: mean/std loaded (shape (158,))\n",
      "meth: mean/std loaded (shape (20114,))\n",
      "\n",
      "Applying training normalization to imputation data...\n",
      "✓ rna normalized using training params\n",
      "✓ rppa normalized using training params\n",
      "✓ meth normalized using training params\n",
      "\n",
      "Data normalization complete.\n",
      "\n",
      "============================================================\n",
      "IMPUTING MISSING MODALITIES (ORDER PRESERVED)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Imputation: 100%|██████████████████████████████████████████████████████████████████████| 38/38 [00:00<00:00, 92.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Denormalizing imputed values...\n",
      "rna: restored to original scale\n",
      "\n",
      "rppa: restored to original scale\n",
      "\n",
      "meth: restored to original scale\n",
      "\n",
      "============================================================\n",
      "MERGING ORIGINAL AND IMPUTED VALUES\n",
      "============================================================\n",
      "rna: NaNs before = 8709380, after = 0\n",
      "rppa: NaNs before = 57442, after = 88\n",
      "meth: NaNs before = 9193530, after = 1432\n",
      "\n",
      "Exporting final CSVs...\n",
      "✓ Saved ./imputed_results/GBM_imputed\\rna.csv  (shape (595, 19660))\n",
      "✓ Saved ./imputed_results/GBM_imputed\\rppa.csv  (shape (595, 158))\n",
      "✓ Saved ./imputed_results/GBM_imputed\\meth.csv  (shape (595, 20114))\n",
      "\n",
      "All imputation complete. Original sample order preserved!\n"
     ]
    }
   ],
   "source": [
    "impute_data_dir = \"./data/GBM_raw\"  # Change this! Points to the dataset for imputation.\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LOADING DATA TO IMPUTE (ORDER-PRESERVING)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Data directory: {impute_data_dir}\\n\")\n",
    "\n",
    "# ---- Load raw data to be imputed (preserving original row order) ----\n",
    "impute_modalities = {}\n",
    "for k, fn in files.items():\n",
    "    p = os.path.join(impute_data_dir, fn)\n",
    "    if os.path.exists(p):\n",
    "        df = load_omics_table(p)\n",
    "\n",
    "        # ⚠ Important: do NOT sort or union — preserve the file’s original row order\n",
    "        impute_modalities[k] = df.copy()\n",
    "\n",
    "        print(f\"✓ {k} loaded from {fn}, shape: {df.shape}\")\n",
    "        print(f\"  → Block-missing rows:  {df.isna().all(axis=1).sum()}\")\n",
    "        print(f\"  → Total missing values: {df.isna().sum().sum()}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Required file not found: {p}\")\n",
    "\n",
    "\n",
    "# ---- Merge sample IDs from all modalities while preserving original order ----\n",
    "def union_preserve_order(list_of_indices):\n",
    "    seen = set()\n",
    "    output = []\n",
    "    for idx_list in list_of_indices:\n",
    "        for x in idx_list:\n",
    "            if x not in seen:\n",
    "                output.append(x)\n",
    "                seen.add(x)\n",
    "    return output\n",
    "\n",
    "# Use raw indices from all modalities, without sorting\n",
    "impute_samples = union_preserve_order([\n",
    "    impute_modalities[k].index.tolist() for k in omics_keys\n",
    "])\n",
    "\n",
    "print(\"\\nTotal samples to impute:\", len(impute_samples))\n",
    "print(\"Sample order preserved.\\n\")\n",
    "\n",
    "\n",
    "# ---- Align all modalities to the same sample order (no sorting) ----\n",
    "impute_aligned = {\n",
    "    k: impute_modalities[k].reindex(impute_samples)\n",
    "    for k in omics_keys\n",
    "}\n",
    "\n",
    "\n",
    "# ---- Extract normalization parameters from training data ----\n",
    "print(\"=\"*60)\n",
    "print(\"EXTRACTING TRAINING NORMALIZATION PARAMS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "normalization_params = {}\n",
    "for k in omics_keys:\n",
    "    train_df = modalities[k]  # Training data modality (original scale)\n",
    "    mu = train_df.mean(axis=0, skipna=True)\n",
    "    sd = train_df.std(axis=0, skipna=True).replace(0, 1.0)\n",
    "    normalization_params[k] = {'mean': mu, 'std': sd}\n",
    "    print(f\"{k}: mean/std loaded (shape {mu.shape})\")\n",
    "\n",
    "\n",
    "# ---- Normalize imputation data using training parameters (order preserved) ----\n",
    "print(\"\\nApplying training normalization to imputation data...\")\n",
    "\n",
    "impute_normalized = {}\n",
    "for k in omics_keys:\n",
    "    mu = normalization_params[k]['mean']\n",
    "    sd = normalization_params[k]['std']\n",
    "    impute_normalized[k] = (impute_aligned[k] - mu) / sd\n",
    "    print(f\"✓ {k} normalized using training params\")\n",
    "\n",
    "\n",
    "# ---- Convert to numpy arrays ----\n",
    "X_impute = {\n",
    "    k: impute_normalized[k].to_numpy(dtype=np.float32)\n",
    "    for k in omics_keys\n",
    "}\n",
    "\n",
    "print(\"\\nData normalization complete.\\n\")\n",
    "\n",
    "\n",
    "# ---- Build dataset without altering sample order ----\n",
    "model.eval()\n",
    "\n",
    "class ImputationDataset(Dataset):\n",
    "    def __init__(self, X_dict):\n",
    "        self.mods = list(X_dict.keys())\n",
    "        self.X = X_dict\n",
    "        self.n = next(iter(X_dict.values())).shape[0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample, observed = {}, {}\n",
    "        for m in self.mods:\n",
    "            x_m = self.X[m][idx]\n",
    "            obs_m = not np.all(np.isnan(x_m))   # block missing?\n",
    "            observed[m] = obs_m\n",
    "            sample[m] = np.nan_to_num(x_m, nan=0.0)\n",
    "        return sample, observed, idx\n",
    "\n",
    "\n",
    "impute_dataset = ImputationDataset(X_impute)\n",
    "impute_loader = DataLoader(impute_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "# ---- Perform imputation ----\n",
    "print(\"=\"*60)\n",
    "print(\"IMPUTING MISSING MODALITIES (ORDER PRESERVED)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "imputed_X = {m: X_impute[m].copy() for m in omics_keys}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sample, observed, indices in tqdm(impute_loader, desc=\"Imputation\"):\n",
    "        batch_x = {m: torch.tensor(sample[m], device=device) for m in sample}\n",
    "        observed_t = {m: torch.tensor(observed[m], device=device) for m in observed}\n",
    "\n",
    "        # Mask: 1 = observed; 0 = block-missing\n",
    "        infer_mask = {m: observed_t[m].float() for m in observed_t}\n",
    "\n",
    "        refined_xhat, _, _ = model(batch_x, infer_mask)\n",
    "\n",
    "        # Write predictions back into imputed_X\n",
    "        for i, global_idx in enumerate(indices):\n",
    "            for m in omics_keys:\n",
    "                if not observed[m][i]:\n",
    "                    imputed_X[m][global_idx] = refined_xhat[m][i].cpu().numpy()\n",
    "\n",
    "\n",
    "# ---- Denormalize results back to original scale ----\n",
    "print(\"\\nDenormalizing imputed values...\")\n",
    "\n",
    "imputed_original_scale = {}\n",
    "for m in omics_keys:\n",
    "    mu = normalization_params[m]['mean'].values\n",
    "    sd = normalization_params[m]['std'].values\n",
    "    restored = imputed_X[m] * sd + mu\n",
    "\n",
    "    imputed_original_scale[m] = pd.DataFrame(\n",
    "        restored,\n",
    "        index=impute_samples,                  # ⚠ Do not reorder\n",
    "        columns=impute_aligned[m].columns      # Column order matches training data\n",
    "    )\n",
    "\n",
    "    print(f\"{m}: restored to original scale\\n\")\n",
    "\n",
    "\n",
    "# ---- Merge original and imputed values (overwrite ONLY NaNs) ----\n",
    "print(\"=\"*60)\n",
    "print(\"MERGING ORIGINAL AND IMPUTED VALUES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "final_data = {}\n",
    "\n",
    "for m in omics_keys:\n",
    "    original = impute_aligned[m]    # Original data (with NaNs)\n",
    "    imputed  = imputed_original_scale[m]\n",
    "\n",
    "    # Keep original observed values; fill only NaNs\n",
    "    final = original.combine_first(imputed)\n",
    "\n",
    "    final_data[m] = final\n",
    "\n",
    "    print(f\"{m}: NaNs before = {original.isna().sum().sum()}, after = {final.isna().sum().sum()}\")\n",
    "\n",
    "\n",
    "# ---- Export results ----\n",
    "output_dir = \"./imputed_results/GBM_imputed\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"\\nExporting final CSVs...\")\n",
    "\n",
    "for m in omics_keys:\n",
    "    out = os.path.join(output_dir, f\"{m}.csv\")\n",
    "    final_data[m].to_csv(out)\n",
    "    print(f\"✓ Saved {out}  (shape {final_data[m].shape})\")\n",
    "\n",
    "print(\"\\nAll imputation complete. Original sample order preserved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928b35ce-d456-4c70-9ebc-f205afa81e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
