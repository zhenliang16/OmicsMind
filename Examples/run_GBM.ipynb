{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acb54a14",
   "metadata": {},
   "source": [
    "# GBM Example\n",
    "\n",
    "\n",
    "This notebook implements the OmicsMind pipeline for GBM dataset:\n",
    "- `rna.csv`\n",
    "- `rppa.csv`\n",
    "- `methylation.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72eb8901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.5.1+cu121\n"
     ]
    }
   ],
   "source": [
    "# !pip -q install pandas numpy scikit-learn tqdm torch\n",
    "\n",
    "import os, random, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from data import load_omics_table, MultiOmicsDataset\n",
    "from omicsmind import ModalityVAE, CrossOmicsTransformer, OmicsMind\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96a3ca9",
   "metadata": {},
   "source": [
    "## 1. Load data tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "760670de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rna loaded (32, 19660)\n",
      "rppa loaded (32, 158)\n",
      "meth loaded (32, 20114)\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"./data/GBM_train\"   \n",
    "files = {\n",
    "    \"rna\": \"rna.csv\",\n",
    "    \"rppa\": \"rppa.csv\",\n",
    "    \"meth\": \"meth.csv\",\n",
    "}\n",
    "\n",
    "modalities = {}\n",
    "missing_files = []\n",
    "for k, fn in files.items():\n",
    "    p = os.path.join(data_dir, fn)\n",
    "    if os.path.exists(p):\n",
    "        modalities[k] = load_omics_table(p)\n",
    "        print(k, \"loaded\", modalities[k].shape)\n",
    "    else:\n",
    "        missing_files.append(p)\n",
    "\n",
    "if missing_files:\n",
    "    print(\"\\n[WARN] Missing files, will use synthetic data:\", missing_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932002a3",
   "metadata": {},
   "source": [
    "## 2. If files are missing, generate synthetic data(Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e03bc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "if missing_files:\n",
    "    np.random.seed(0)\n",
    "    n = 80\n",
    "    modalities = {\n",
    "        \"rna\":  pd.DataFrame(np.random.randn(n, 2000), index=[f\"S{i:03d}\" for i in range(n)]),\n",
    "        \"rppa\": pd.DataFrame(np.random.randn(n, 200),  index=[f\"S{i:03d}\" for i in range(n)]),\n",
    "        \"meth\": pd.DataFrame(np.random.randn(n, 1000), index=[f\"S{i:03d}\" for i in range(n)]),\n",
    "    }\n",
    "    # Simulate block-wise missingness\n",
    "    idx = modalities[\"rna\"].index\n",
    "    mask_rppa = np.random.choice(idx, size=int(0.3*n), replace=False)\n",
    "    mask_meth = np.random.choice(idx, size=int(0.2*n), replace=False)\n",
    "    modalities[\"rppa\"].loc[mask_rppa] = np.nan\n",
    "    modalities[\"meth\"].loc[mask_meth] = np.nan\n",
    "\n",
    "    print(\"Synthetic modalities:\", {k:v.shape for k,v in modalities.items()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4668d29f",
   "metadata": {},
   "source": [
    "## 3. Sample alignment + z-score normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eb226bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rna': (32, 19660), 'rppa': (32, 158), 'meth': (32, 20114)}\n"
     ]
    }
   ],
   "source": [
    "omics_keys = list(files.keys())\n",
    "all_samples = sorted(set().union(*[modalities[k].index for k in omics_keys]))\n",
    "\n",
    "def align(df): \n",
    "    return df.reindex(all_samples)\n",
    "\n",
    "modalities_aligned = {k: align(modalities[k]) for k in omics_keys}\n",
    "\n",
    "def zscore(df):\n",
    "    mu = df.mean(axis=0, skipna=True)\n",
    "    sd = df.std(axis=0, skipna=True).replace(0, 1.0)\n",
    "    return (df - mu) / sd\n",
    "\n",
    "modalities_aligned = {k: zscore(v) for k,v in modalities_aligned.items()}\n",
    "\n",
    "X = {k: v.to_numpy(dtype=np.float32) for k,v in modalities_aligned.items()}\n",
    "print({k: X[k].shape for k in X})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc64492",
   "metadata": {},
   "source": [
    "## 4. Dataset for block-wise masked-modality training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a49422ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MultiOmicsDataset(X, mask_prob=0.3)\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06da24cd",
   "metadata": {},
   "source": [
    "## 6. Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70e7c704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | loss=2.9548 rec=2.9409 kl=13.8795\n",
      "Epoch 005 | loss=2.9470 rec=2.8983 kl=48.6249\n",
      "Epoch 010 | loss=2.9044 rec=2.8815 kl=22.8435\n",
      "Epoch 015 | loss=2.8045 rec=2.7738 kl=30.6660\n",
      "Epoch 020 | loss=2.6982 rec=2.6484 kl=49.7596\n",
      "Epoch 025 | loss=2.6334 rec=2.5752 kl=58.1564\n",
      "Epoch 030 | loss=2.5407 rec=2.4710 kl=69.7205\n",
      "Epoch 035 | loss=2.5755 rec=2.5064 kl=69.0572\n",
      "Epoch 040 | loss=2.3389 rec=2.2408 kl=98.0977\n",
      "Epoch 045 | loss=2.3164 rec=2.2216 kl=94.8031\n",
      "Epoch 050 | loss=2.2668 rec=2.1502 kl=116.6294\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "in_dims = {m: X[m].shape[1] for m in X}\n",
    "\n",
    "model = OmicsMind(in_dims, z_dim=64).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def to_tensor_batch(batch):\n",
    "    sample, observed, train_mask = batch\n",
    "    batch_x = {m: torch.tensor(sample[m], device=device) for m in sample}\n",
    "    observed_t = {m: torch.tensor(observed[m], device=device) for m in observed}\n",
    "    train_mask_t = {m: torch.tensor(train_mask[m], device=device) for m in train_mask}\n",
    "    return batch_x, observed_t, train_mask_t\n",
    "\n",
    "epochs = 50  # increase to 200+ for paper-level runs\n",
    "for ep in range(1, epochs+1):\n",
    "    model.train()\n",
    "    total = total_rec = total_kl = 0.0\n",
    "    for batch in loader:\n",
    "        bx, obs_t, mask_t = to_tensor_batch(batch)\n",
    "        refined_xhat, mus, logvars = model(bx, mask_t)\n",
    "        loss, rec, kl = omicsmind_loss(bx, refined_xhat, mus, logvars, obs_t, mask_t, only_masked=False)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total += loss.item(); total_rec += rec.item(); total_kl += kl.item()\n",
    "\n",
    "    if ep % 5 == 0 or ep == 1:\n",
    "        print(f\"Epoch {ep:03d} | loss={total/len(loader):.4f} rec={total_rec/len(loader):.4f} kl={total_kl/len(loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dea188",
   "metadata": {},
   "source": [
    "## 9. Full imputation and CSV export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b5e766c-8c7f-442b-9759-69c0ebb12b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOADING DATA TO IMPUTE (ORDER-PRESERVING)\n",
      "============================================================\n",
      "Data directory: ./data/GBM_raw\n",
      "\n",
      "✓ rna loaded from rna.csv, shape: (595, 19660)\n",
      "  → Block-missing rows:  443\n",
      "  → Total missing values: 8709380\n",
      "✓ rppa loaded from rppa.csv, shape: (595, 158)\n",
      "  → Block-missing rows:  363\n",
      "  → Total missing values: 57442\n",
      "✓ meth loaded from meth.csv, shape: (595, 20114)\n",
      "  → Block-missing rows:  457\n",
      "  → Total missing values: 9193530\n",
      "\n",
      "Total samples to impute: 595\n",
      "Sample order preserved.\n",
      "\n",
      "============================================================\n",
      "EXTRACTING TRAINING NORMALIZATION PARAMS\n",
      "============================================================\n",
      "rna: mean/std loaded (shape (19660,))\n",
      "rppa: mean/std loaded (shape (158,))\n",
      "meth: mean/std loaded (shape (20114,))\n",
      "\n",
      "Applying training normalization to imputation data...\n",
      "✓ rna normalized using training params\n",
      "✓ rppa normalized using training params\n",
      "✓ meth normalized using training params\n",
      "\n",
      "Data normalization complete.\n",
      "\n",
      "============================================================\n",
      "IMPUTING MISSING MODALITIES (ORDER PRESERVED)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Imputation: 100%|█████████████████████████████████████████████| 38/38 [00:00<00:00, 49.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Denormalizing imputed values...\n",
      "rna: restored to original scale\n",
      "\n",
      "rppa: restored to original scale\n",
      "\n",
      "meth: restored to original scale\n",
      "\n",
      "============================================================\n",
      "MERGING ORIGINAL AND IMPUTED VALUES\n",
      "============================================================\n",
      "rna: NaNs before = 8709380, after = 0\n",
      "rppa: NaNs before = 57442, after = 88\n",
      "meth: NaNs before = 9193530, after = 1432\n",
      "\n",
      "Exporting final CSVs...\n",
      "✓ Saved ./imputed_results/GBM_imputed\\rna.csv  (shape (595, 19660))\n",
      "✓ Saved ./imputed_results/GBM_imputed\\rppa.csv  (shape (595, 158))\n",
      "✓ Saved ./imputed_results/GBM_imputed\\meth.csv  (shape (595, 20114))\n",
      "\n",
      "All imputation complete. Original sample order preserved!\n"
     ]
    }
   ],
   "source": [
    "impute_data_dir = \"./data/GBM_raw\"  # Change this! Points to the dataset for imputation.\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LOADING DATA TO IMPUTE (ORDER-PRESERVING)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Data directory: {impute_data_dir}\\n\")\n",
    "\n",
    "# ---- Load raw data to be imputed (preserving original row order) ----\n",
    "impute_modalities = {}\n",
    "for k, fn in files.items():\n",
    "    p = os.path.join(impute_data_dir, fn)\n",
    "    if os.path.exists(p):\n",
    "        df = load_omics_table(p)\n",
    "\n",
    "        # ⚠ Important: do NOT sort or union — preserve the file’s original row order\n",
    "        impute_modalities[k] = df.copy()\n",
    "\n",
    "        print(f\"✓ {k} loaded from {fn}, shape: {df.shape}\")\n",
    "        print(f\"  → Block-missing rows:  {df.isna().all(axis=1).sum()}\")\n",
    "        print(f\"  → Total missing values: {df.isna().sum().sum()}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Required file not found: {p}\")\n",
    "\n",
    "\n",
    "# ---- Merge sample IDs from all modalities while preserving original order ----\n",
    "def union_preserve_order(list_of_indices):\n",
    "    seen = set()\n",
    "    output = []\n",
    "    for idx_list in list_of_indices:\n",
    "        for x in idx_list:\n",
    "            if x not in seen:\n",
    "                output.append(x)\n",
    "                seen.add(x)\n",
    "    return output\n",
    "\n",
    "# Use raw indices from all modalities, without sorting\n",
    "impute_samples = union_preserve_order([\n",
    "    impute_modalities[k].index.tolist() for k in omics_keys\n",
    "])\n",
    "\n",
    "print(\"\\nTotal samples to impute:\", len(impute_samples))\n",
    "print(\"Sample order preserved.\\n\")\n",
    "\n",
    "\n",
    "# ---- Align all modalities to the same sample order (no sorting) ----\n",
    "impute_aligned = {\n",
    "    k: impute_modalities[k].reindex(impute_samples)\n",
    "    for k in omics_keys\n",
    "}\n",
    "\n",
    "\n",
    "# ---- Extract normalization parameters from training data ----\n",
    "print(\"=\"*60)\n",
    "print(\"EXTRACTING TRAINING NORMALIZATION PARAMS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "normalization_params = {}\n",
    "for k in omics_keys:\n",
    "    train_df = modalities[k]  # Training data modality (original scale)\n",
    "    mu = train_df.mean(axis=0, skipna=True)\n",
    "    sd = train_df.std(axis=0, skipna=True).replace(0, 1.0)\n",
    "    normalization_params[k] = {'mean': mu, 'std': sd}\n",
    "    print(f\"{k}: mean/std loaded (shape {mu.shape})\")\n",
    "\n",
    "\n",
    "# ---- Normalize imputation data using training parameters (order preserved) ----\n",
    "print(\"\\nApplying training normalization to imputation data...\")\n",
    "\n",
    "impute_normalized = {}\n",
    "for k in omics_keys:\n",
    "    mu = normalization_params[k]['mean']\n",
    "    sd = normalization_params[k]['std']\n",
    "    impute_normalized[k] = (impute_aligned[k] - mu) / sd\n",
    "    print(f\"✓ {k} normalized using training params\")\n",
    "\n",
    "\n",
    "# ---- Convert to numpy arrays ----\n",
    "X_impute = {\n",
    "    k: impute_normalized[k].to_numpy(dtype=np.float32)\n",
    "    for k in omics_keys\n",
    "}\n",
    "\n",
    "print(\"\\nData normalization complete.\\n\")\n",
    "\n",
    "\n",
    "# ---- Build dataset without altering sample order ----\n",
    "model.eval()\n",
    "\n",
    "class ImputationDataset(Dataset):\n",
    "    def __init__(self, X_dict):\n",
    "        self.mods = list(X_dict.keys())\n",
    "        self.X = X_dict\n",
    "        self.n = next(iter(X_dict.values())).shape[0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample, observed = {}, {}\n",
    "        for m in self.mods:\n",
    "            x_m = self.X[m][idx]\n",
    "            obs_m = not np.all(np.isnan(x_m))   # block missing?\n",
    "            observed[m] = obs_m\n",
    "            sample[m] = np.nan_to_num(x_m, nan=0.0)\n",
    "        return sample, observed, idx\n",
    "\n",
    "\n",
    "impute_dataset = ImputationDataset(X_impute)\n",
    "impute_loader = DataLoader(impute_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "# ---- Perform imputation ----\n",
    "print(\"=\"*60)\n",
    "print(\"IMPUTING MISSING MODALITIES (ORDER PRESERVED)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "imputed_X = {m: X_impute[m].copy() for m in omics_keys}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sample, observed, indices in tqdm(impute_loader, desc=\"Imputation\"):\n",
    "        batch_x = {m: torch.tensor(sample[m], device=device) for m in sample}\n",
    "        observed_t = {m: torch.tensor(observed[m], device=device) for m in observed}\n",
    "\n",
    "        # Mask: 1 = observed; 0 = block-missing\n",
    "        infer_mask = {m: observed_t[m].float() for m in observed_t}\n",
    "\n",
    "        refined_xhat, _, _ = model(batch_x, infer_mask)\n",
    "\n",
    "        # Write predictions back into imputed_X\n",
    "        for i, global_idx in enumerate(indices):\n",
    "            for m in omics_keys:\n",
    "                if not observed[m][i]:\n",
    "                    imputed_X[m][global_idx] = refined_xhat[m][i].cpu().numpy()\n",
    "\n",
    "\n",
    "# ---- Denormalize results back to original scale ----\n",
    "print(\"\\nDenormalizing imputed values...\")\n",
    "\n",
    "imputed_original_scale = {}\n",
    "for m in omics_keys:\n",
    "    mu = normalization_params[m]['mean'].values\n",
    "    sd = normalization_params[m]['std'].values\n",
    "    restored = imputed_X[m] * sd + mu\n",
    "\n",
    "    imputed_original_scale[m] = pd.DataFrame(\n",
    "        restored,\n",
    "        index=impute_samples,                  # ⚠ Do not reorder\n",
    "        columns=impute_aligned[m].columns      # Column order matches training data\n",
    "    )\n",
    "\n",
    "    print(f\"{m}: restored to original scale\\n\")\n",
    "\n",
    "\n",
    "# ---- Merge original and imputed values (overwrite ONLY NaNs) ----\n",
    "print(\"=\"*60)\n",
    "print(\"MERGING ORIGINAL AND IMPUTED VALUES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "final_data = {}\n",
    "\n",
    "for m in omics_keys:\n",
    "    original = impute_aligned[m]    # Original data (with NaNs)\n",
    "    imputed  = imputed_original_scale[m]\n",
    "\n",
    "    # Keep original observed values; fill only NaNs\n",
    "    final = original.combine_first(imputed)\n",
    "\n",
    "    final_data[m] = final\n",
    "\n",
    "    print(f\"{m}: NaNs before = {original.isna().sum().sum()}, after = {final.isna().sum().sum()}\")\n",
    "\n",
    "\n",
    "# ---- Export results ----\n",
    "output_dir = \"./imputed_results/GBM_imputed\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"\\nExporting final CSVs...\")\n",
    "\n",
    "for m in omics_keys:\n",
    "    out = os.path.join(output_dir, f\"{m}.csv\")\n",
    "    final_data[m].to_csv(out)\n",
    "    print(f\"✓ Saved {out}  (shape {final_data[m].shape})\")\n",
    "\n",
    "print(\"\\nAll imputation complete. Original sample order preserved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928b35ce-d456-4c70-9ebc-f205afa81e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bio12]",
   "language": "python",
   "name": "conda-env-bio12-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
